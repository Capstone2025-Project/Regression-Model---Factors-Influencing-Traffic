# -*- coding: utf-8 -*-
"""Regression model Factors importance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQHvBC-0K8YalK1gbCgINdYjp9b8N8Mo
"""

from google.colab import files

import warnings, os, json, math
warnings.filterwarnings("ignore")
print("ðŸ“¤ Please upload Final Dataset.csv ...")
uploaded = files.upload()

# replicate_with_coefficients_MODIFIED.py
# Azure-style ensemble + Ridge (with coefficients) for Total_Traffic prediction
# Requirements: pandas, numpy, scikit-learn>=1.3, xgboost, joblib, matplotlib

import pandas as pd
import numpy as np
from pathlib import Path
import os
import io # Added for handling the image data structure if needed, but not strictly for a file

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold, cross_validate
from sklearn.metrics import make_scorer, r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge
import joblib

from xgboost import XGBRegressor
import matplotlib.pyplot as plt

# --- ASSUMPTION ---
# I am assuming the code will be run with 'Final Dataset.csv' which has the columns shown in the image.
# For the purpose of providing executable code, I will use a dummy DataFrame that matches the image structure
# in place of pd.read_csv(CSV_PATH) for demonstration, but keep the original file path setup.

# Ensure 'outputs' directory exists for saving files
Path("outputs").mkdir(parents=True, exist_ok=True)
print("Output directory 'outputs/' ensured.")

# -----------------------------
# 1) Config (MODIFIED FEATURE LISTS)
# -----------------------------
CSV_PATH = "Final Dataset.csv" # Placeholder for the actual file
TARGET = "Total_Traffic"

DATE_COL = "Date" # This column is present in the image data
# Updated Categorical Columns based on the image
CAT_COLS = ["Facility", "Year", "Season", "Month", "Day_Name", "Is_Holiday", "Holiday_Name"]
# Updated Numerical Columns based on the image (The column names are slightly different)
NUM_COLS = [
    "Total_Violations",
    "Avg_WindSpeed",
    "Avg_Precipitation", # This is 'Avg_Precipitation' in the image
    "Avg_Snowfall",
    "Avg_SnowDepth",
    "Avg_Temperature_Max",
    "Avg_Temperature_Min",
]
ALL_FEATURES = [DATE_COL] + CAT_COLS + NUM_COLS

# -----------------------------
# 2) Load data (Modified for Robustness against missing columns/types)
# -----------------------------
print("\n[2] Loading Data...")
# --- START OF MODIFICATION FOR DEMO/TESTING IF FILE IS NOT PRESENT ---
# Since I cannot load the actual 'Final Dataset.csv', I will create a dummy DF
# based on the image's structure for the code to be runnable and verified.
try:
    df = pd.read_csv(CSV_PATH)
except FileNotFoundError:
    print(f"File '{CSV_PATH}' not found. Using dummy data structure from image for execution.")
    # Creating a DataFrame that matches the image's structure
    data = {
        'Facility': ['Bayonne'] * 20,
        'Date': pd.to_datetime(['9/19/2016', '9/20/2016', '9/21/2016', '9/22/2016', '9/23/2016',
                                '9/24/2016', '9/25/2016', '9/26/2016', '9/27/2016', '9/28/2016',
                                '9/29/2016', '9/30/2016', '10/1/2016', '10/2/2016', '10/3/2016',
                                '10/4/2016', '10/5/2016', '10/6/2016', '10/7/2016', '10/8/2016']),
        'Year': [2016] * 20,
        'Season': ['Autumn'] * 20,
        'Month': ['September'] * 12 + ['October'] * 8,
        'Day_Name': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',
                     'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',
                     'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'],
        'Is_Holiday': [0] * 14 + [1] + [0] * 5, # Example 'Is_Holiday' column
        'Holiday_Name': ['NA'] * 14 + ['Columbus Day'] + ['NA'] * 5,
        'Total_Violations': [51, 142, 152, 149, 137, 105, 60, 155, 113, 116, 136, 147, 16, 112, 126, 112, 125, 138, 184, 223],
        'Avg_WindSpeed': [2.24, 2.46, 3.8, 2.46, 1.79, 2.91, 2.91, 4.47, 3.13, 8.05, 11.18, 9.62, 7.61, 2.68, 2.24, 7.61, 4.7, 2.01, 2.46, 2.01],
        'Avg_Precipitation': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        'Avg_Snowfall': [0] * 20,
        'Avg_SnowDepth': [0] * 20,
        'Avg_Temperature_Max': [76, 82, 84, 86, 87, 81, 70, 74, 74, 69, 64, 59, 62, 63, 72, 69, 67, 73, 75, 68],
        'Avg_Temperature_Min': [69, 68, 68, 65, 64, 62, 54, 54, 56, 52, 57, 56, 56, 57, 60, 60, 53, 55, 57, 58],
        'Total_Traffic': [1714, 5145, 5438, 5413, 5589, 2911, 2911, 5153, 5111, 5207, 5248, 5904, 646, 4446, 4609, 4598, 5170, 5308, 7533, 6265]
    }
    df = pd.DataFrame(data)
# --- END OF MODIFICATION FOR DEMO/TESTING IF FILE IS NOT PRESENT ---

missing = [c for c in [TARGET] + ALL_FEATURES if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

df = df[[TARGET] + ALL_FEATURES].copy()
df[TARGET] = pd.to_numeric(df[TARGET], errors="coerce")
df = df[~df[TARGET].isna()].reset_index(drop=True)

if not np.issubdtype(df[DATE_COL].dtype, np.datetime64):
    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors="coerce")

X = df[ALL_FEATURES].copy()
y = df[TARGET].values
print(f"Data loaded. X shape: {X.shape}, y shape: {y.shape}")

# -----------------------------
# 3) Preprocessing (No changes, uses the defined feature lists)
# -----------------------------
def expand_date(X_in: pd.DataFrame) -> pd.DataFrame:
    d = pd.to_datetime(X_in[DATE_COL].squeeze())
    out = pd.DataFrame(
        {
            "date_year": d.dt.year,
            "date_month": d.dt.month,
            "date_day": d.dt.day,
            "date_dayofweek": d.dt.dayofweek,
            "date_weekofyear": d.dt.isocalendar().week.astype("Int64"),
        },
        index=X_in.index,
    )
    return out

def get_date_feature_names_out(X, feature_names_in):
    """Returns the expected column names created by expand_date."""
    return np.array([
        "date_year",
        "date_month",
        "date_day",
        "date_dayofweek",
        "date_weekofyear"
    ], dtype=object)

date_transformer = Pipeline(
    steps=[
        ("extract", FunctionTransformer(
            expand_date,
            validate=False,
            feature_names_out=get_date_feature_names_out
        )),
        ("impute", SimpleImputer(strategy="most_frequent")),
        ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

cat_transformer = Pipeline(
    steps=[
        ("impute", SimpleImputer(strategy="most_frequent")),
        ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
    ]
)

num_transformer = Pipeline(
    steps=[
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler(with_mean=False)), # StandardScaler with_mean=False for sparse matrices
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("date", date_transformer, [DATE_COL]),
        ("cat", cat_transformer, CAT_COLS),
        ("num", num_transformer, NUM_COLS),
    ],
    remainder="drop",
    sparse_threshold=0.3,
)

# -----------------------------
# 4) Ensemble Definition (No changes)
# -----------------------------
xgb_params = {
    "min_child_weight": 1, "tree_method": "hist",
    "random_state": 42, "n_jobs": -1
}

xgb0 = XGBRegressor(learning_rate=0.2, max_depth=10, colsample_bytree=0.7, gamma=0.1, subsample=0.8, n_estimators=500, **xgb_params)
xgb1 = XGBRegressor(learning_rate=0.3, max_depth=6, colsample_bytree=0.8, gamma=0.1, subsample=0.8, n_estimators=600, **xgb_params)
xgb2 = XGBRegressor(learning_rate=0.3, max_depth=6, colsample_bytree=1.0, gamma=0.0, subsample=0.9, n_estimators=400, **xgb_params)
xgb3 = XGBRegressor(learning_rate=0.2, max_depth=8, colsample_bytree=0.9, gamma=0.0, subsample=0.9, n_estimators=600, **xgb_params)
xgb4 = XGBRegressor(learning_rate=0.5, max_depth=6, colsample_bytree=1.0, gamma=0.0, subsample=0.8, n_estimators=400, **xgb_params)
xgb5 = XGBRegressor(learning_rate=0.3, max_depth=8, colsample_bytree=1.0, gamma=0.0, subsample=0.8, n_estimators=500, **xgb_params)
xgb6 = XGBRegressor(learning_rate=0.5, max_depth=10, colsample_bytree=0.9, gamma=0.1, subsample=0.9, n_estimators=300, **xgb_params)


ensemble = VotingRegressor(
    estimators=[
        ("model_0", xgb0), ("model_1", xgb1), ("model_2", xgb2),
        ("model_3", xgb3), ("model_4", xgb4), ("model_5", xgb5),
        ("model_6", xgb6),
    ],
    weights=[1/3, 1/3, 1/15, 1/15, 1/15, 1/15, 1/15],
)

ensemble_pipe = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", ensemble),
    ]
)

# -----------------------------
# 5) Ridge Definition (No changes)
# -----------------------------
ridge = Ridge(alpha=1.0, random_state=42)
ridge_pipe = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", ridge),
    ]
)

# -----------------------------
# 6) Cross-validate both (No changes)
# -----------------------------
print("\n[6] Running Cross-Validation...")
scorers = {
    "r2": make_scorer(r2_score),
    "mae": make_scorer(mean_absolute_error, greater_is_better=False),
    "rmse": make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False),
}
cv = KFold(n_splits=5, shuffle=True, random_state=42)

def report_cv(name, pipe):
    cv_res = cross_validate(pipe, X, y, scoring=scorers, cv=cv, n_jobs=-1, return_train_score=False)
    print(f"\n{name} CV:")
    # Store metrics for final output
    metrics = {
        'R2': f"{cv_res['test_r2'].mean():,.4f}",
        'MAE': f"{-cv_res['test_mae'].mean():,.2f}",
        'RMSE': f"{-cv_res['test_rmse'].mean():,.2f}"
    }
    print(" Â  Â R2 Â  Â :", metrics['R2'])
    print(" Â  Â MAE Â  :", metrics['MAE'])
    print(" Â  Â RMSE Â :", metrics['RMSE'])
    return cv_res, metrics

cv_ens, ensemble_metrics = report_cv("Ensemble", ensemble_pipe)
cv_ridge, ridge_metrics = report_cv("Ridge", ridge_pipe)

# -----------------------------
# 7) Fit final models (No changes)
# -----------------------------
print("\n[7] Fitting final models on full dataset...")
ensemble_pipe.fit(X, y)
ridge_pipe.fit(X, y)

joblib.dump(ensemble_pipe, "outputs/azure_ensemble_replica.joblib")
joblib.dump(ridge_pipe, "outputs/ridge_coeff_model.joblib")

# -----------------------------
# 8) Extract and List ALL Coefficients (No changes)
# -----------------------------
print("\n[8] Extracting and Analyzing Ridge Coefficients...")
feature_names = ridge_pipe.named_steps["preprocessor"].get_feature_names_out()
coefs = ridge_pipe.named_steps["model"].coef_.ravel()
intercept = ridge_pipe.named_steps["model"].intercept_

coef_df = pd.DataFrame({"feature": feature_names, "coefficient": coefs})
coef_df["abs_coeff"] = coef_df["coefficient"].abs()
coef_df = coef_df.sort_values("abs_coeff", ascending=False).reset_index(drop=True)

print("\n=========================================================")
print("             COMPLETE RIDGE COEFFICIENT LIST (Top 20)            ")
print("=========================================================")
print(f"Model Intercept (Baseline Total_Traffic): {intercept:,.4f}")
print(f"Total Coefficients: {len(coefs)}\n")

print(coef_df[['feature', 'coefficient']].head(20).to_string(index=False))

# Save the full list
coef_df.to_csv("outputs/ridge_feature_coefficients_detailed.csv", index=False)
print("\nFull coefficient list saved to outputs/ridge_feature_coefficients_detailed.csv")

# -----------------------------
# 9) Extract and List SEASON Coefficients (No changes)
# -----------------------------
print("\n=========================================================")
print("                     SEASON COEFFICIENTS                     ")
print("=========================================================")

season_coef_df = coef_df[coef_df['feature'].str.startswith('cat__Season_')]
season_coef_df = season_coef_df.sort_values('coefficient', ascending=False)

print(season_coef_df[['feature', 'coefficient']].to_string(index=False))

# -----------------------------
# 10) Final Grouped Analysis and Plotting (No changes)
# -----------------------------

def base_from_feature_name(n: str) -> str:
    if n.startswith("num__"):
        return n.split("__", 1)[1]
    if n.startswith("cat__"):
        rest = n.split("__", 1)[1]
        return rest.split("_", 1)[0]
    if n.startswith("date__"):
        rest = n.split("__", 1)[1]
        rest = rest.split("__", 1)[-1]
        parts = rest.split("_")
        return "_".join(parts[:2]) if len(parts) >= 2 else rest
    return n

coef_df["base_feature"] = coef_df["feature"].map(base_from_feature_name)

grouped = (
    coef_df.groupby("base_feature", as_index=False)
           .agg(total_abs_coeff=("abs_coeff", "sum"),
                max_abs_coeff=("abs_coeff", "max"),
                n_terms=("abs_coeff", "size"))
           .sort_values("total_abs_coeff", ascending=False)
           .reset_index(drop=True)
)

print("\nGrouped by original feature (sum |abs(coeff)|):")
print(grouped.head(20).to_string(index=False))
grouped.to_csv("outputs/ridge_feature_coefficients_grouped.csv", index=False)

# Plotting logic remains here...
top_factors = grouped.head(10).copy()
# Prepare for barplot 1: Top 5 variables
top5_factors = grouped.head(5).copy()
total_sum_top5 = top5_factors["total_abs_coeff"].sum()
top5_factors["normalized_coeff"] = top5_factors["total_abs_coeff"] / total_sum_top5

# Plot 1: Top 5 Feature Importance Factors
plt.figure(figsize=(8, 5))
plt.bar(
    x=top5_factors["base_feature"],
    height=top5_factors["normalized_coeff"],
    color='#1f77b4', # A nice blue
    align='center'
)
plt.title("Top 5 Feature Importance Factors (Normalized Absolute Ridge Coefficients)")
plt.ylabel("Relative Importance (Normalized)")
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1.0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plot_path_top5 = "outputs/ridge_top5_factors_barplot.png"
plt.savefig(plot_path_top5)
print(f"\nSaved plot: {plot_path_top5}")
#  # Add relevant image tag

# Plot 2: Top 10 Feature Importance Factors (Original)
total_sum = top_factors["total_abs_coeff"].sum()
top_factors["normalized_coeff"] = top_factors["total_abs_coeff"] / total_sum

plt.figure(figsize=(10, 6))
plt.bar(
    x=top_factors["base_feature"],
    height=top_factors["normalized_coeff"],
    color='gray',
    align='center'
)
plt.title("Top 10 Feature Importance Factors (Normalized Absolute Ridge Coefficients)")
plt.ylabel("Relative Importance (Normalized)")
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1.0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plot_path_top10 = "outputs/ridge_top10_factors_barplot.png"
plt.savefig(plot_path_top10)
print(f"\nSaved plot: {plot_path_top10}")
#  # Add relevant image tag

# --- Code to generate the plot of ALL 56 coefficients ---
plt.figure(figsize=(14, 8)) # Wider figure to accommodate many bars

# Sort the coefficients by value for better visualization
plot_data = coef_df.sort_values("coefficient", ascending=False)

# Create the bar plot
plt.bar(
    x=plot_data["feature"],
    height=plot_data["coefficient"],
    color=np.where(plot_data["coefficient"] > 0, '#2ca02c', '#d62728'), # Green for positive, Red for negative
    align='center'
)

plt.axhline(0, color='black', linewidth=0.8) # Add a line at y=0 for reference
plt.title("Individual Ridge Coefficients for Total_Traffic Prediction (Total: 56)", fontsize=16)
plt.ylabel("Coefficient Value (Impact on Total_Traffic)", fontsize=12)
plt.xlabel("Feature Name", fontsize=12)
plt.xticks(rotation=90, ha='right', fontsize=8) # Rotate labels and use smaller font
plt.tight_layout()

plot_path_all_coefs = "outputs/ridge_all_56_coefficients_plot.png"
plt.savefig(plot_path_all_coefs)
print(f"\nSaved plot of ALL coefficients: {plot_path_all_coefs}")

# --- Code Snippet to print the full coefficient table ---
print("\n=========================================================")
print("             FULL RIDGE COEFFICIENT TABLE (All 56)             ")
print("=========================================================")

# Display the entire coefficient DataFrame
# Display up to 60 rows to ensure all 56 are visible if you run this in a notebook/console
print(coef_df[['feature', 'coefficient', 'abs_coeff']].head(60).to_string(index=False))